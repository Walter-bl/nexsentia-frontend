# NexSentia AI Chatbot - Backend API Specification

## Overview

This document specifies the exact format for backend responses to the chatbot frontend. Follow these specifications to ensure proper streaming and rendering of AI responses.

---

## API Endpoints

### 1. Streaming Endpoint (Recommended)

**POST** `/chatbot/chat-stream`

**Headers:**
```
Authorization: Bearer {JWT_TOKEN}
Content-Type: application/json
```

**Request Body:**
```json
{
  "message": "Show me critical weak signals",
  "sessionId": "550e8400-e29b-41d4-a716-446655440000",
  "conversationHistory": [
    {
      "role": "user",
      "content": "What's our organizational health?",
      "timestamp": "2024-02-11T10:00:00Z"
    },
    {
      "role": "assistant",
      "content": "Your health score is 72/100...",
      "timestamp": "2024-02-11T10:00:05Z"
    }
  ]
}
```

**Response:** Server-Sent Events (SSE) stream

**Content-Type:** `text/event-stream`

---

## SSE Event Format

### Event 1: Metadata (First Event)

Send this immediately when streaming starts:

```
data: {"type":"metadata","sessionId":"550e8400-e29b-41d4-a716-446655440000","sources":{"signals":10,"incidents":5,"issues":8,"metrics":1}}

```

**Format:**
```typescript
{
  "type": "metadata",
  "sessionId": string,        // UUID for this conversation
  "sources": {                // Data sources used (optional)
    "signals": number,        // Number of signals analyzed
    "incidents": number,      // Number of incidents checked
    "issues": number,         // Number of issues reviewed
    "metrics": number         // Number of metrics computed
  }
}
```

---

### Event 2: Token Events (Multiple)

Send one event per word/token as the AI generates the response:

```
data: {"type":"token","content":"## "}

data: {"type":"token","content":"Critical "}

data: {"type":"token","content":"Weak "}

data: {"type":"token","content":"Signals\n\n"}

data: {"type":"token","content":"I "}

data: {"type":"token","content":"found "}

data: {"type":"token","content":"**3 "}

data: {"type":"token","content":"critical** "}

data: {"type":"token","content":"signals...\n"}

```

**Format:**
```typescript
{
  "type": "token",
  "content": string           // The word/token to append
}
```

**Important Notes:**
- Send tokens as they're generated by OpenAI
- Include markdown formatting in tokens (e.g., `**`, `##`, `\n`)
- Include newlines (`\n`) for proper formatting
- Each line must end with `\n\n` (two newlines for SSE format)

---

### Event 3: Done Event (Last Event)

Send this when the response is complete:

```
data: {"type":"done"}

```

**Format:**
```typescript
{
  "type": "done"
}
```

---

## Complete Streaming Example

```http
HTTP/1.1 200 OK
Content-Type: text/event-stream
Cache-Control: no-cache
Connection: keep-alive

data: {"type":"metadata","sessionId":"550e8400-e29b-41d4-a716-446655440000","sources":{"signals":5,"incidents":3}}

data: {"type":"token","content":"## "}

data: {"type":"token","content":"Critical "}

data: {"type":"token","content":"Weak "}

data: {"type":"token","content":"Signals\n\n"}

data: {"type":"token","content":"I "}

data: {"type":"token","content":"found "}

data: {"type":"token","content":"**3 "}

data: {"type":"token","content":"critical "}

data: {"type":"token","content":"signals** "}

data: {"type":"token","content":"requiring "}

data: {"type":"token","content":"attention:\n\n"}

data: {"type":"token","content":"1. "}

data: {"type":"token","content":"**Communication "}

data: {"type":"token","content":"Breakdown** "}

data: {"type":"token","content":"- "}

data: {"type":"token","content":"Engineering "}

data: {"type":"token","content":"Team\n"}

data: {"type":"done"}

```

---

## Response Content Format (Markdown)

The content sent in token events should be **markdown-formatted text**. Here's how to structure different response types:

---

### Format 1: Signal Summary

**Markdown Structure:**
```markdown
## Critical Weak Signals

I found **{count} critical signals** requiring attention:

### ðŸ”´ High Priority

1. **{Signal Title}** - {Department}
   - **Severity:** `{HIGH/MEDIUM/LOW}`
   - **Impact Score:** {score}/10
   - **Detected:** {date}
   - **Affected:** {count} team members

   > **Analysis:** {brief analysis of the signal}

2. **{Next Signal}**...

---

### Recommendations

- **Immediate:** {action item}
- **This Week:** {action item}
- **Monitor:** {tracking item}

[View Signal Dashboard](/dashboard/signals)
```

**Example Output:**
```markdown
## Critical Weak Signals

I found **3 critical signals** requiring attention:

### ðŸ”´ High Priority

1. **Communication Breakdown** - Engineering Team
   - **Severity:** `HIGH`
   - **Impact Score:** 8.5/10
   - **Detected:** 2024-02-08
   - **Affected:** 15 team members

   > **Analysis:** 40% decrease in cross-team communication detected over the past 2 weeks.

2. **Deadline Pressure** - Product Sprint
   - **Severity:** `HIGH`
   - **Impact Score:** 7.8/10
   - **Risk:** Project delay by 2-3 weeks

---

### Recommendations

- **Immediate:** Schedule team sync meeting
- **This Week:** Review resource allocation
- **Monitor:** Track communication metrics daily
```

---

### Format 2: Organizational Health Report

**Markdown Structure:**
```markdown
## Organizational Health Report

Your current health score is **{score}/100** (â†“ {change} points from last week)

### Key Metrics

| Metric | Score | Change | Status |
|--------|-------|--------|--------|
| Strategic Alignment | {score} | â†“ -{percent}% | ðŸŸ¡ Warning |
| Communication Efficiency | {score} | â†“ -{percent}% | ðŸ”´ Critical |
| Team Morale | {score} | â†‘ +{percent}% | ðŸŸ¢ Good |

### Insights

**Positive Trends:**
- {trend 1}
- {trend 2}

**Areas of Concern:**
- {concern 1}
- {concern 2}

> **Recommendation:** {actionable advice}
```

---

### Format 3: Incident Report

**Markdown Structure:**
```markdown
## Top Incidents - Last 7 Days

I analyzed **{count} incidents** from {date range}.

### Critical Incidents

**1. {Incident Title}**
- **Status:** âœ… Resolved / ðŸŸ¡ In Progress
- **Duration:** {duration}
- **Impact:** {user count} users affected
- **Root Cause:** {cause}

```{language}
{code block if applicable}
```

**Resolution:** {how it was fixed}

---

### Incident Statistics

| Category | Count | Avg Resolution Time |
|----------|-------|---------------------|
| Infrastructure | {count} | {time} |
| Application | {count} | {time} |

### Recommendations

1. {action item 1}
2. {action item 2}
```

---

### Format 4: Simple Response

**Markdown Structure:**
```markdown
{Brief answer to the question}

**Key Points:**
- {point 1}
- {point 2}
- {point 3}

{Additional context if needed}
```

---

### Format 5: Error/No Data Response

**Markdown Structure:**
```markdown
## No {Data Type} Found

I couldn't find any {data type} matching your criteria.

**Possible reasons:**
- {reason 1}
- {reason 2}

**Try this instead:**
- {suggestion 1}
- {suggestion 2}
```

---

## Backend Implementation Examples

### Node.js + Express Example

```javascript
const express = require('express');
const { OpenAI } = require('openai');

const app = express();
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

app.post('/chatbot/chat-stream', async (req, res) => {
  const { message, sessionId, conversationHistory } = req.body;

  // Set SSE headers
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');

  try {
    // 1. Fetch relevant data from your database
    const signals = await fetchWeakSignals();
    const incidents = await fetchIncidents();
    const metrics = await fetchMetrics();

    // 2. Send metadata event
    const metadata = {
      type: 'metadata',
      sessionId: sessionId || generateUUID(),
      sources: {
        signals: signals.length,
        incidents: incidents.length,
        metrics: metrics.length
      }
    };
    res.write(`data: ${JSON.stringify(metadata)}\n\n`);

    // 3. Build context for OpenAI
    const context = buildContext(signals, incidents, metrics);

    // 4. Create OpenAI streaming request
    const systemPrompt = `You are NexSentia AI Assistant. You help users understand organizational health, weak signals, incidents, and metrics.

IMPORTANT: Format all responses using markdown:
- Use ## for main headings, ### for subheadings
- Use **bold** for emphasis on important data
- Use tables for data comparisons
- Use bullet points for lists
- Use code blocks for technical data
- Use blockquotes (>) for important recommendations
- Always provide actionable insights

Available data:
${context}
`;

    const messages = [
      { role: 'system', content: systemPrompt },
      ...conversationHistory,
      { role: 'user', content: message }
    ];

    const stream = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: messages,
      stream: true,
      temperature: 0.7,
    });

    // 5. Stream tokens to client
    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content || '';
      if (content) {
        const tokenEvent = {
          type: 'token',
          content: content
        };
        res.write(`data: ${JSON.stringify(tokenEvent)}\n\n`);
      }
    }

    // 6. Send done event
    res.write(`data: ${JSON.stringify({ type: 'done' })}\n\n`);
    res.end();

  } catch (error) {
    console.error('Streaming error:', error);
    res.write(`data: ${JSON.stringify({
      type: 'error',
      message: error.message
    })}\n\n`);
    res.end();
  }
});

// Helper functions
function buildContext(signals, incidents, metrics) {
  return `
Weak Signals (${signals.length}):
${signals.map(s => `- ${s.title} (Severity: ${s.severity}, Impact: ${s.impact})`).join('\n')}

Recent Incidents (${incidents.length}):
${incidents.map(i => `- ${i.title} (Status: ${i.status}, Duration: ${i.duration})`).join('\n')}

Key Metrics:
${Object.entries(metrics).map(([k, v]) => `- ${k}: ${v}`).join('\n')}
`;
}

function generateUUID() {
  return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {
    const r = Math.random() * 16 | 0;
    const v = c === 'x' ? r : (r & 0x3 | 0x8);
    return v.toString(16);
  });
}

async function fetchWeakSignals() {
  // Query your database for signals
  // Example:
  const signals = await db.query(`
    SELECT id, title, severity, impact_score, detected_date
    FROM weak_signals
    WHERE status = 'active'
    ORDER BY severity DESC, impact_score DESC
    LIMIT 10
  `);
  return signals;
}

async function fetchIncidents() {
  // Query incidents from database
  const incidents = await db.query(`
    SELECT id, title, status, severity, created_at, resolved_at
    FROM incidents
    WHERE created_at >= NOW() - INTERVAL '7 days'
    ORDER BY severity DESC, created_at DESC
    LIMIT 20
  `);
  return incidents;
}

async function fetchMetrics() {
  // Compute or fetch metrics
  return {
    healthScore: 72,
    strategicAlignment: 78,
    communicationEfficiency: 65,
    teamMorale: 71
  };
}

app.listen(3000, () => {
  console.log('Chatbot API running on port 3000');
});
```

---

### Python + FastAPI Example

```python
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from openai import OpenAI
import json
import uuid
from datetime import datetime

app = FastAPI()
openai_client = OpenAI(api_key="your-api-key")

@app.post("/chatbot/chat-stream")
async def chat_stream(request: Request):
    body = await request.json()
    message = body.get('message')
    session_id = body.get('sessionId', str(uuid.uuid4()))
    conversation_history = body.get('conversationHistory', [])

    async def event_generator():
        try:
            # 1. Fetch data
            signals = await fetch_weak_signals()
            incidents = await fetch_incidents()

            # 2. Send metadata
            metadata = {
                "type": "metadata",
                "sessionId": session_id,
                "sources": {
                    "signals": len(signals),
                    "incidents": len(incidents)
                }
            }
            yield f"data: {json.dumps(metadata)}\n\n"

            # 3. Build context
            context = build_context(signals, incidents)

            # 4. Create OpenAI stream
            system_prompt = f"""You are NexSentia AI Assistant. Format responses with markdown.

Available data:
{context}
"""

            messages = [
                {"role": "system", "content": system_prompt},
                *conversation_history,
                {"role": "user", "content": message}
            ]

            stream = openai_client.chat.completions.create(
                model="gpt-4",
                messages=messages,
                stream=True,
                temperature=0.7
            )

            # 5. Stream tokens
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    token_event = {
                        "type": "token",
                        "content": chunk.choices[0].delta.content
                    }
                    yield f"data: {json.dumps(token_event)}\n\n"

            # 6. Send done event
            yield f"data: {json.dumps({'type': 'done'})}\n\n"

        except Exception as e:
            error_event = {
                "type": "error",
                "message": str(e)
            }
            yield f"data: {json.dumps(error_event)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }
    )

def build_context(signals, incidents):
    context = f"Weak Signals ({len(signals)}):\n"
    for signal in signals:
        context += f"- {signal['title']} (Severity: {signal['severity']})\n"

    context += f"\nRecent Incidents ({len(incidents)}):\n"
    for incident in incidents:
        context += f"- {incident['title']} (Status: {incident['status']})\n"

    return context

async def fetch_weak_signals():
    # Query database
    return [
        {"id": "WS-001", "title": "Communication Breakdown", "severity": "HIGH"},
        {"id": "WS-002", "title": "Deadline Pressure", "severity": "HIGH"}
    ]

async def fetch_incidents():
    # Query database
    return [
        {"id": "INC-001", "title": "Database Performance", "status": "Resolved"},
        {"id": "INC-002", "title": "API Timeout", "status": "In Progress"}
    ]
```

---

## OpenAI System Prompt Template

Use this system prompt to ensure OpenAI formats responses correctly:

```javascript
const systemPrompt = `You are NexSentia AI Assistant, an AI that helps users understand organizational health, weak signals, incidents, and performance metrics.

FORMATTING RULES:
1. Always use markdown formatting for responses
2. Use ## for main section headings
3. Use ### for subsection headings
4. Use **bold** for important metrics, numbers, and key terms
5. Use tables for comparing multiple data points
6. Use bullet points for lists
7. Use code blocks for technical data (JSON, SQL, etc.)
8. Use blockquotes (>) for important recommendations or alerts
9. Use inline code (\`text\`) for IDs, status values, and short technical terms
10. Include emojis sparingly: ðŸ”´ (critical), ðŸŸ¡ (warning), ðŸŸ¢ (good), âœ… (resolved)

RESPONSE STRUCTURE:
1. Start with a clear heading (##)
2. Provide a brief summary (1-2 sentences)
3. Break down details with subheadings (###)
4. Use tables for metrics/comparisons
5. End with actionable recommendations
6. Include relevant links when applicable

TONE:
- Professional but conversational
- Data-driven and specific
- Actionable and helpful
- Concise but complete

AVAILABLE DATA:
${contextData}

Remember: User cannot see raw data. Present insights in human-readable format with proper context.
`;
```

---

## Response Examples by Query Type

### Query: "Show me critical weak signals"

**Formatted Response:**
```markdown
## Critical Weak Signals

I found **3 critical signals** requiring immediate attention:

### ðŸ”´ High Priority

1. **Communication Breakdown** - Engineering Team
   - **Severity:** \`HIGH\`
   - **Impact Score:** 8.5/10
   - **Detected:** 2024-02-08
   - **Affected:** 15 team members

   > **Analysis:** 40% decrease in cross-team communication over 2 weeks.

2. **Deadline Pressure** - Product Sprint
   - **Severity:** \`HIGH\`
   - **Impact Score:** 7.8/10
   - **Risk:** Project delay by 2-3 weeks

---

### Recommendations

- **Immediate:** Schedule emergency team sync
- **This Week:** Review resource allocation
- **Monitor:** Track communication metrics daily
```

---

### Query: "What's our health score?"

**Formatted Response:**
```markdown
## Organizational Health Report

Your current health score is **72/100** (â†“ 8 points from last week)

### Key Metrics

| Metric | Score | Change | Status |
|--------|-------|--------|--------|
| Strategic Alignment | 78 | â†“ -5% | ðŸŸ¡ Warning |
| Communication | 65 | â†“ -12% | ðŸ”´ Critical |
| Team Morale | 71 | â†‘ +3% | ðŸŸ¢ Good |
| Velocity | 80 | â†’ 0% | ðŸŸ¢ Good |

### Analysis

**Areas of Concern:**
- Communication efficiency dropped significantly (-12%)
- 3 escalated incidents this week vs 1 last week

> **Recommendation:** Address communication gaps immediately through team retrospective.
```

---

### Query: "Top incidents this week"

**Formatted Response:**
```markdown
## Top Incidents - Last 7 Days

I analyzed **12 incidents** from Feb 5-11, 2024.

### Critical Incidents

**1. Database Performance Degradation**
- **Status:** âœ… Resolved
- **Duration:** 2h 32m
- **Impact:** 500+ users
- **Resolution:** Added missing index

\`\`\`sql
CREATE INDEX idx_user_activity
ON user_logs(user_id, created_at);
\`\`\`

**2. API Gateway Timeout**
- **Status:** ðŸŸ¡ In Progress
- **Duration:** 6+ hours
- **ETA:** 2 hours

### Statistics

| Category | Count | Avg Resolution |
|----------|-------|----------------|
| Infrastructure | 5 | 3.2 hours |
| Application | 7 | 5.1 hours |
```

---

## Testing Your Implementation

### Test Checklist

1. **Metadata Event**
   - [ ] Sent first before any tokens
   - [ ] Contains valid UUID sessionId
   - [ ] Sources object is present

2. **Token Events**
   - [ ] Tokens stream in real-time
   - [ ] Markdown formatting preserved
   - [ ] Newlines (`\n`) included correctly
   - [ ] No missing or corrupted characters

3. **Done Event**
   - [ ] Sent after all tokens
   - [ ] Connection closed properly

4. **Content Format**
   - [ ] Headings render correctly
   - [ ] Tables display properly
   - [ ] Code blocks formatted
   - [ ] Lists show with bullets
   - [ ] Bold/italic text works

### Test Queries

Send these test queries to verify formatting:

1. `"Show me critical weak signals"` - Should return structured list
2. `"What's our organizational health?"` - Should return table with metrics
3. `"Top incidents this week"` - Should return numbered list with details
4. `"Show me signal WS-001 in JSON"` - Should return code block

---

## Error Handling

### Error Response Format

If an error occurs, send an error event:

```javascript
{
  "type": "error",
  "message": "Failed to fetch signals",
  "code": "DATABASE_ERROR"
}
```

The frontend will display this as an error message to the user.

---

## Performance Considerations

1. **Token Size:** Send tokens as OpenAI provides them (usually 1-4 characters)
2. **Flush Immediately:** Don't buffer tokens, send immediately
3. **Timeout:** Implement 30-second timeout for streaming
4. **Connection Keep-Alive:** Keep SSE connection open until done
5. **Database Queries:** Fetch all data before streaming starts

---

## Security

1. **Authentication:** Always verify JWT token before processing
2. **Rate Limiting:** Limit to 10 requests per minute per user
3. **Input Validation:** Sanitize user input before sending to OpenAI
4. **Data Access:** Only return data user has permission to see
5. **Session Management:** Store sessions securely with expiration

---

## Summary

**Required Format:**
1. SSE with `text/event-stream` content type
2. Three event types: `metadata`, `token`, `done`
3. Markdown-formatted content in tokens
4. Proper newlines for SSE (`\n\n` after each event)

**Content Guidelines:**
- Use markdown for all formatting
- Structure with headings, tables, lists
- Include metrics and data
- Provide actionable recommendations
- Keep responses concise but complete

**Implementation:**
- Stream tokens as OpenAI generates them
- Send metadata first, done last
- Handle errors gracefully
- Close connection properly

---

**Ready for implementation! ðŸš€**

Any questions? Refer to the example code above or the formatting guide for details.
